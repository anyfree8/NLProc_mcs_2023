{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import re\n","\n","from sklearn import datasets\n","from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"],"metadata":{"id":"r3Kg-M1EMZCZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ppw4nIaYsQEG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### TF-IDF\n","$TF$ - частота встречания терма (term frequency)<br>\n","$IDF$ - обратная частота встречания терма в документах (invert document frequency)\n","\n","$$tf_{i, j} = \\frac{N_i}{\\sum\\limits_j N_j}$$<br>\n","(1) $$idf_{i} = log(\\frac{|D|}{|\\{d_k \\in D | i \\in d_k \\}|}) \\approx log (\\frac{|D|}{df(i) + 1}) \\approx log (\\frac{|D| + 1}{df(i) + 1}) + 1$$\n","\n","(2) $$idf_{i} = log(\\frac{|D|}{|\\{d_k \\in D | i \\in d_k \\}|}) + 1$$\n"],"metadata":{"id":"hVJIQn7LPcxR"}},{"cell_type":"markdown","source":["### Это ознакомительное задание. В нём от вас требуется\n","    \n","* Задать правильную формулу TF.IDF в функции `__calc_token_weight`,\n","* Передать в неё правильные параметры.\n","\n","_NB! В этой задаче в словарях с частотами бывает удобно использовать_ `defaultdict` _или_ `Counter`"],"metadata":{"id":"DK0NpKzzMOTT"}},{"cell_type":"code","source":["from collections import defaultdict, deque, Counter"],"metadata":{"id":"_jpNeJ4y4-BG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import math"],"metadata":{"id":"pURpnTxYIMpH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9BZ-1skaL-bS"},"outputs":[],"source":["class TfIdfExtractor:\n","    def __init__(self, ngrams=1):\n","        self.token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n","        self.ngrams = ngrams\n","\n","        self.docs_with_token = None\n","        self.token_hits_count_in_doc = None\n","        self.token_count_in_doc = None\n","        self.docs_count = None\n","\n","    def compute_counts_in_collection_collection(self, documents):\n","        \"\"\"\n","        Заполняем словари (хэшмэпы, ассоциативные массивы) разных частот для последующего вычисления tf-idf\n","        :param documents: сырые тексты\n","        \"\"\"\n","        self.docs_with_token, self.token_hits_count_in_doc, self.token_count_in_doc = self.__count_tokens(documents)\n","        self.docs_count = len(documents)\n","\n","    def __count_tokens(self, documents):\n","        \"\"\"\n","        Реализуйте метод, который строит словари (хэшмэпы, ассоциативные массивы) разных частот для последующего вычисления tf-idf.\n","        Здесь не требуется никаких трюков, тренируемся считать явно.\n","\n","        :param documents: сырые тексты\n","        :return: кортеж-тройка:\n","        docs_with_token,            # словарь: docs_with_token[токен] = число содержащих его документов\n","        token_hits_count_in_doc,    # словарь: token_hits_count_in_doc[документ][токен] = сколько раз токен встратился в данном документе\n","        token_count_in_doc          # словарь: token_count_in_doc[документ] = сколько всего токенов в документе\n","        \"\"\"\n","\n","        docs_with_token = defaultdict(int)\n","        token_hits_count_in_doc = []\n","        token_count_in_doc = []\n","\n","        for doc in documents:\n","\n","            # частоты токенов в этом документе\n","            token_dictionary = defaultdict(int)\n","            token_count = 0\n","\n","            for token in self.__get_ngrams(doc):\n","                #token = token.lower()\n","                # считаем частоты отдельных токенов\n","                if token_dictionary[token] == 0:\n","                  token_count += 1\n","                token_dictionary[token] += 1\n","\n","\n","            token_hits_count_in_doc.append(token_dictionary)\n","            token_count_in_doc.append(token_count)\n","            # для каждого токена ведём учёт числа документов, в которых он встретился\n","            for token in token_dictionary.keys():\n","              docs_with_token[token] += 1\n","              \n","\n","        return docs_with_token, token_hits_count_in_doc, token_count_in_doc\n","\n","    def __get_ngrams(self, doc):\n","        \"\"\"\n","        Функция, вычисляющая tf-idf, для которого всё подготовлено.\n","        При необходимости используйте функцию findall() у поля self.token_pattern и английские стоп-слова.\n","\n","        :param doc: документ\n","        :return: tokens список, состоящий из n-грам\n","        \"\"\"\n","        tokens = []\n","        clear_doc = [word for word in self.token_pattern.findall(doc)\n","                    if word not in ENGLISH_STOP_WORDS]\n","\n","        window = deque()\n","        for word in clear_doc:\n","          word = word.lower()\n","          if len(window) == self.ngrams:\n","            tokens.append(\"_\".join(window))\n","            window.popleft()\n","          window.append(word)\n","\n","        return tokens\n","\n","    def __calc_token_weight(self, document_count, docs_with_token, tokens_in_doc, tokens_total):\n","        \"\"\"\n","        Метод, вычисляющий tf-idf, для которого всё подготовлено.\n","        При необходимости используйте math.log.\n","\n","        :param document_count: общее число документов\n","        :param docs_with_token: число документов\n","        :param tokens_in_doc: сколько раз токен встретился в документе\n","        :param tokens_total: общее число токенов в данном документе\n","        :return: tf.idf\n","        \"\"\"\n","\n","        idf = math.log(document_count / docs_with_token) + 1\n","        tf = tokens_in_doc / tokens_total\n","        return tf * idf\n","\n","    def extract_tfidf(self, topn_docs):\n","        \"\"\"\n","        Считаем топ по tf-idf для первых N документов.\n","\n","        :param topn_docs: число документов из списка\n","        :return: extracted_tfidf список пар <токен, tf-idf>, отсортированных по найденным tf-idf\n","        \"\"\"\n","\n","        extracted_tfidf = []\n","        for n_doc in range(topn_docs):\n","            token_weights = {}\n","\n","            for token in self.token_hits_count_in_doc[n_doc]:\n","                token_weights[token] = self.__calc_token_weight(self.docs_count,\n","                                                          # число документов, содержащих token\n","                                                          self.docs_with_token[token],\n","                                                          # сколько раз встретился токен token в документе n_doc\n","                                                          self.token_hits_count_in_doc[n_doc][token],\n","                                                          # сколько токенов в документе n_doc\n","                                                          self.token_count_in_doc[n_doc])\n","\n","            # сортируем по tf-idf\n","            desc_tfidf_tokens = sorted(token_weights.items(), key=lambda x: (x[1], x[0]), reverse=True)\n","            extracted_tfidf.append(desc_tfidf_tokens)\n","\n","        return extracted_tfidf"]},{"cell_type":"markdown","source":["Загрузим наш первый датасет"],"metadata":{"id":"mIIKM8Wpoyaj"}},{"cell_type":"code","source":["# \"истинные\" наиболее представительные слова в первых 10 документах\n","reference = [\n","    \"cheaper supply orbits reach economies c5hcbo alike allen ground repairstation\",\n","    \"reston overhead wrap allen leadership fee integration dennis centers nasa\",\n","    \"revolt grasp alaska files acad3 prograsm geta cshow autocad 124722\",\n","    \"list rankings krumenaker 71160 2356 source larry compuserve traffic unsubscribed\",\n","    \"class foundation banquet teachers dinner teaching studies space lichtenberg planetary\",\n","    \"access muc hicksville flaking expecting redundancy navstar bird pat digex\",\n","    \"lehigh children abominable tfv0 ucdavis wealth starving capital games dan\",\n","    \"processors silicon lower slower ssrt higher access future hjistorical germanium\",\n","    \"wpi maverick giaquinto worcester novice 2280 01609 information shuttles periodicals\",\n","    \"accelerations henry toronto acceleration andrew immersion generalizes endured efpx7ws00uh7qaop1s zoology\"\n","]"],"metadata":{"id":"dr6IZsIlTGsY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CHECKED_DOCS = len(reference)\n","CHECKED_TOP = len(reference[0].split(\" \"))"],"metadata":{"id":"yxGj9fbyTHpf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CHECKED_DOCS, CHECKED_TOP"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cjyAwRwHTNhI","outputId":"e5374e01-9ee8-4c33-acf4-46fb0233e4e6","executionInfo":{"status":"ok","timestamp":1677239189892,"user_tz":-180,"elapsed":3,"user":{"displayName":"Даниил Ануфриев","userId":"03049540340862951945"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10, 10)"]},"metadata":{},"execution_count":140}]},{"cell_type":"code","source":["# один из стандартных наборов данных для классификации текстов\n","newsgroups = datasets.fetch_20newsgroups(subset='all', categories=['sci.space'])\n","\n","# тексты\n","documents = newsgroups.data"],"metadata":{"id":"lj9GPqvFTk-Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Запустим подсчёт TFIDF сначала на униграммах"],"metadata":{"id":"mnXQ5QNpn0OP"}},{"cell_type":"code","source":["extractor = TfIdfExtractor(ngrams=1)\n","extractor.compute_counts_in_collection_collection(documents)"],"metadata":{"id":"4Vn4LQ7vM_u4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["precision_accumulator = 0.0\n","extracted_tfidf = extractor.extract_tfidf(CHECKED_DOCS)\n","\n","# посчитаем топ по tf-idf для первых десяти документов\n","for n_doc in range(CHECKED_DOCS):\n","    # сортируем по tf-idf\n","    desc_tfidf_tokens = extracted_tfidf[n_doc]\n","    tfidf_top = list(map(lambda x: x[0], desc_tfidf_tokens))[:CHECKED_TOP]\n","\n","    # можно распечатать и посмотреть, отличается ли порядок (не должен)\n","    print(\" \".join(tfidf_top))\n","    print(reference[n_doc])\n","\n","    # вычисляем что-то вроде Precision@10\n","    tfidf_top_set = set(tfidf_top)\n","    in_reference = len(tfidf_top_set.intersection(set(reference[n_doc].split(\" \"))))\n","    precision = in_reference / CHECKED_TOP\n","\n","    print(n_doc, \"Precision: %.1f%%\" % (precision * 100.0))\n","    precision_accumulator += precision\n","\n","# необходимое, но не достаточное условие: у правильного решения здесь должно быть ровно 100%\n","print(\"Avg precision: %.2f%%\" % (float(precision_accumulator) / CHECKED_DOCS * 100.0))"],"metadata":{"id":"SEVaD6CbMwr6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677239202863,"user_tz":-180,"elapsed":228,"user":{"displayName":"Даниил Ануфриев","userId":"03049540340862951945"}},"outputId":"0049acce-dc2d-4536-cd29-43987c0d8302"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cheaper supply orbits reach economies c5hcbo alike allen ground repairstation\n","cheaper supply orbits reach economies c5hcbo alike allen ground repairstation\n","0 Precision: 100.0%\n","reston overhead wrap allen leadership fee integration dennis centers nasa\n","reston overhead wrap allen leadership fee integration dennis centers nasa\n","1 Precision: 100.0%\n","revolt grasp alaska files acad3 prograsm geta cshow autocad 124722\n","revolt grasp alaska files acad3 prograsm geta cshow autocad 124722\n","2 Precision: 100.0%\n","list rankings krumenaker 71160 2356 source larry compuserve traffic unsubscribed\n","list rankings krumenaker 71160 2356 source larry compuserve traffic unsubscribed\n","3 Precision: 100.0%\n","class foundation banquet teachers dinner teaching studies space lichtenberg planetary\n","class foundation banquet teachers dinner teaching studies space lichtenberg planetary\n","4 Precision: 100.0%\n","access muc hicksville flaking expecting redundancy navstar bird digex positions\n","access muc hicksville flaking expecting redundancy navstar bird pat digex\n","5 Precision: 90.0%\n","lehigh children abominable tfv0 ucdavis wealth starving capital games dan\n","lehigh children abominable tfv0 ucdavis wealth starving capital games dan\n","6 Precision: 100.0%\n","processors silicon lower slower ssrt higher access future hjistorical germanium\n","processors silicon lower slower ssrt higher access future hjistorical germanium\n","7 Precision: 100.0%\n","wpi maverick giaquinto worcester novice 2280 01609 information shuttles periodicals\n","wpi maverick giaquinto worcester novice 2280 01609 information shuttles periodicals\n","8 Precision: 100.0%\n","accelerations toronto henry acceleration andrew immersion generalizes endured efpx7ws00uh7qaop1s zoology\n","accelerations henry toronto acceleration andrew immersion generalizes endured efpx7ws00uh7qaop1s zoology\n","9 Precision: 100.0%\n","Avg precision: 99.00%\n"]}]},{"cell_type":"markdown","source":["Теперь прогоним наш подсчёт на биграммах"],"metadata":{"id":"PHWgexg0oId2"}},{"cell_type":"code","source":["# \"истинные\" наиболее представительные словосочетания в первых 10 документах\n","reference = [\n","    'satellite_orbit supply_make supply_facility sources_cheaper source_supply scale_allen repair_supply remember_presence ready_source reached_ready',\n","    'large_scale wrap_overhead work_it stated_wrap lack_leadership wrong_nasa successful_large single_important scale_effort reston_nasa',\n","    'alaska_edu acad3_alaska think_cshow space_design slide_shows shuttle_design shows_think revolt_look revolt_if related_items',\n","    'larry_krumenaker 71160_2356 2356_compuserve compuserve_com won_answer unsubscribed_list unfortunately_did traffic_rankings traffic_given temporarily_unsubscribed',\n","    'studies_foundation planetary_studies dinner_banquet the_class teaching_newest space_teaching newest_frontier hours_graduate graduate_credit foundation_the',\n","    'users_areas soon_orbit redundancy_plane provide_redundancy orbit_major orbit_hicksville net_bird needed_provide muc_user major_users',\n","    'lehigh_edu ucdavis_edu workers_end wo_man wealth_way way_ist viewpoint_subject tv_companies trying_everyone thats_abominable',\n","    'computer_technology what_argument weight_ssrt times_lower throw_hjistorical this_kind think_dc theoretically_future technology_stated technology_actually',\n","    'wpi_wpi wpi_edu maverick_wpi space_program worcester_ma todd_giaquinto suggest_books subject_general sites_novice shuttles_history',\n","    'andrew_cmu water_immersion violent_deceleration using_water think_30 talking_sustained sustained_acceleration still_higher sounds_bit odd_gees'\n","]"],"metadata":{"id":"pFvyreZ-okXU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bigram_extractor = TfIdfExtractor(ngrams=2)\n","bigram_extractor.compute_counts_in_collection_collection(documents)"],"metadata":{"id":"_Mile9HZX7pV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["precision_accumulator = 0.0\n","extracted_tfidf = bigram_extractor.extract_tfidf(CHECKED_DOCS)\n","\n","for n_doc in range(CHECKED_DOCS):\n","    desc_tfidf_tokens = extracted_tfidf[n_doc]\n","    tfidf_top = list(map(lambda x: x[0], desc_tfidf_tokens))[:CHECKED_TOP]\n","\n","    tfidf_top_set = set(tfidf_top)\n","    in_reference = len(tfidf_top_set.intersection(set(reference[n_doc].split(\" \"))))\n","    precision = in_reference / CHECKED_TOP\n","\n","    print(n_doc, \"Precision: %.1f%%\" % (precision * 100.0))\n","    precision_accumulator += precision\n","\n","print(\"Avg precision: %.2f%%\" % (float(precision_accumulator) / CHECKED_DOCS * 100.0))"],"metadata":{"id":"TDtnhQiyX7j9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677239239451,"user_tz":-180,"elapsed":3,"user":{"displayName":"Даниил Ануфриев","userId":"03049540340862951945"}},"outputId":"8ce1defb-163c-496e-d377-59da1a6a23e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 Precision: 100.0%\n","1 Precision: 100.0%\n","2 Precision: 100.0%\n","3 Precision: 100.0%\n","4 Precision: 100.0%\n","5 Precision: 100.0%\n","6 Precision: 100.0%\n","7 Precision: 100.0%\n","8 Precision: 100.0%\n","9 Precision: 100.0%\n","Avg precision: 100.00%\n"]}]},{"cell_type":"markdown","source":["А теперь попробуйте посчитать tf-idf на любом значении параметра `ngrams` "],"metadata":{"id":"PEeN8-zDXdHb"}},{"cell_type":"code","source":["ngram_extractor = TfIdfExtractor(ngrams=3)\n","ngram_extractor.compute_counts_in_collection_collection(documents)"],"metadata":{"id":"N-7EDe_cX7JN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["precision_accumulator = 0.0\n","extracted_tfidf = ngram_extractor.extract_tfidf(CHECKED_DOCS)\n","\n","for n_doc in range(CHECKED_DOCS):\n","    desc_tfidf_tokens = extracted_tfidf[n_doc]\n","    tfidf_top = list(map(lambda x: x[0], desc_tfidf_tokens))[:CHECKED_TOP]\n","    print(f'TOP-{CHECKED_TOP} important words for {n_doc}th document: {tfidf_top}')"],"metadata":{"id":"tJrdiybGX7fh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677239278851,"user_tz":-180,"elapsed":4,"user":{"displayName":"Даниил Ануфриев","userId":"03049540340862951945"}},"outputId":"a5e2f7f3-c787-418b-9853-f3f8e17e76d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TOP-10 important words for 0th document: ['writes_the_biggest', 'value_space_if', 'the_biggest_problem', 'supply_make_cheaper', 'supply_facility_adds', 'space_if_satellite', 'space_based_sources', 'sources_cheaper_reach', 'source_supply_make', 'scale_allen_lady']\n","TOP-10 important words for 1th document: ['successful_large_scale', 'single_important_successful', 'large_scale_effort', 'important_successful_large', 'charge_fee_you', 'wingo_cspara_decnet', 'judy_uh_edu', 'fedex_msfc_nasa', 'decnet_fedex_msfc', 'cspara_decnet_fedex']\n","TOP-10 important words for 2th document: ['acad3_alaska_edu', 'think_cshow_liek', 'subject_space_design', 'station_designs_finished', 'space_shuttle_design', 'space_related_items', 'space_design_movies', 'slide_shows_think', 'shuttle_design_autocad', 'shows_think_cshow']\n","TOP-10 important words for 3th document: ['71160_2356_compuserve', '2356_compuserve_com', 'won_answer_larry', 'unsubscribed_list_cut', 'unfortunately_did_clip', 'traffic_rankings_listserv', 'traffic_given_unfortunately', 'temporarily_unsubscribed_list', 'subject_ranking_space', 'stuffing_reply_mail']\n","TOP-10 important words for 4th document: ['planetary_studies_foundation', 'teaching_newest_frontier', 'studies_foundation_the', 'space_teaching_newest', 'hours_graduate_credit', 'united_states_space', 'states_space_foundation', 'ecs_comm_mot', 'dennisn_ecs_comm', 'comm_mot_com']\n","TOP-10 important words for 5th document: ['users_areas_needed', 'soon_orbit_major', 'redundancy_plane_orbit', 'provide_redundancy_plane', 'positions_organization_express', 'plane_orbit_hicksville', 'orbit_major_users', 'orbit_hicksville_muc', 'net_bird_flaking', 'needed_provide_redundancy']\n","TOP-10 important words for 6th document: ['writes_deleted_is', 'workers_end_result', 'wo_man_altruism', 'wealth_way_ist', 'wealth_help_dan', 'way_ist_clinto', 'viewpoint_subject_abominations', 'university_lines_37', 'tv_companies_sponsors', 'trying_everyone_starve']\n","TOP-10 important words for 7th document: ['what_argument_new', 'weight_ssrt_ssrt', 'times_lower_costs', 'throw_hjistorical_framework', 'thought_throw_hjistorical', 'this_kind_speculative', 'think_dc_fit', 'theoretically_future_higher', 'technology_stated_new', 'technology_actually_lower']\n","TOP-10 important words for 8th document: ['maverick_wpi_wpi', 'wpi_wpi_edu', 'wpi_edu_looking', 'wpi_edu_giaquinto', 'worcester_ma_01609', 'todd_giaquinto_maverick', 'this_includes_nasa', 'suggest_books_periodicals', 'subject_general_information', 'space_program_todd']\n","TOP-10 important words for 9th document: ['andrew_cmu_edu', 'writes_question_finally', 'water_immersion_doubt', 'violent_deceleration_if', 'using_water_immersion', 'this_sounds_bit', 'think_30_odd', 'talking_sustained_acceleration', 'sustained_acceleration_think', 'still_higher_accelerations']\n"]}]}]}